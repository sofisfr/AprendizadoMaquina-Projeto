---
title: "Projeto Final"
author: |
        | Nome: Sofia Ferreira
        | E-mail: sofia.f@aluno.ufabc.edu.br
        
        | Nome: Pedro Henrique Donassan de Macedo
        | E-mail: pedro.donassan@aluno.ufabc.edu.br
        
        | Nome: 
        | E-mail: 
        
        | Nome: 
        | E-mail: 
        
        | Nome: 
        | E-mail: 
        
        | Nome: 
        | E-mail: 
        | (Não é preciso informar os RAs)
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T,
                      fig.align='center',
                      cache=TRUE,
                      out.width = "60%",
                      out.heigth = "60%",
                      warning=FALSE,
                      message=FALSE)
options(width =70)
```

#

```{r pt. 1, include=TRUE, echo=TRUE}

## Introdução


```

```{r Libraries}
library(tidyverse)
library(tidymodels)
library(mlbench)
library(GGally)
library(corrplot)
library(vip)
library(janitor)

library(rpart.plot)
library(randomForest)
```


```{r pt. 2 , include=TRUE, echo=TRUE}

data <- read.csv(
    "Sleep_health_and_lifestyle_dataset.csv", 
    header=TRUE, 
    sep = ",", 
    row.names = NULL, 
    quote = "\"") 

data <- data %>% clean_names() #Formatando nome das colunas

data <- separate(data, blood_pressure, into = c("systolic_pressure", "diastolic_pressure"), sep = "/") #Transformando pressão sanguínea em 2 variáveis numericas
data$systolic_pressure <- as.numeric(data$systolic_pressure)
data$diastolic_pressure <- as.numeric(data$diastolic_pressure)

data$gender <- as.factor(data$gender)
data$occupation <- as.factor(data$occupation)
data$bmi_category <- as.factor(data$bmi_category)
#data$blood_pressure <- as.factor(data$blood_pressure)
data$sleep_disorder <- as.factor(data$sleep_disorder)

SleepData <- as_tibble(data) 
ggpairs(SleepData, columns = c("age", "sleep_duration", "quality_of_sleep", "physical_activity_level", "stress_level", "heart_rate", "daily_steps", "systolic_pressure", "diastolic_pressure"))


data %>% na.omit() %>% select_if(is.numeric) %>% cor() %>% corrplot::corrplot(tl.cex = 0.5) #Printando matriz de correlação
```
```{r}

#Split para todos os modelos
set.seed(123) # para reprodutibilidade
data_split <- initial_split(SleepData, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)

#Cv para todos os modelos
set.seed(123) # para reprodutibilidade
cv_folds <- vfold_cv(train_data, v = 5) # 5 Folds pois o dataset é pequeno

#Recipe para todos os modelos
rec <- recipe(sleep_disorder ~ ., data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>% 
  step_rm(person_id) %>%  ### Remove a coluna person ID que é inutil para o modelo
  step_corr(all_numeric()) ### Remove variaveis altamente correlacionadas

```

```{r pt. 3 , include=TRUE, echo=TRUE}

## Explicação algoritmos
cat("Florestas Aleatórias\n")


cat("Florestas Aleatórias é um algoritmo de aprendizado de máquina que opera construindo uma multiplicidade de árvores de decisão durante o tempo de treinamento. É um método de aprendizado de conjunto para classificação, regressão e outras tarefas. Para tarefas de classificação, a saída do algortimo é a classe selecionada pela maioria das árvores. Florestas Aleatórias geralmente superam as árvores de decisão, mas sua precisão é menor do que as árvores impulsionadas pelo gradiente.\n\n")


cat("XGBoost - Extreme Gradient Boosting\n")

cat("O XGBoost é uma implementação otimizada do algoritmo de gradient boosting, uma técnica que combina várias árvores de decisão fracas para formar um modelo forte. Cada árvore subsequente é treinada para corrigir os erros do modelo anterior. A combinação das previsões de todas as árvores individuais resulta em uma previsão final robusta e precisa. \n")

cat("XGBoost é uma escolha sólida para problemas de classificação devido à sua eficiência, desempenho, capacidade de lidar com complexidade e regularização embutida, o que resulta em modelos mais precisos e robustos.")
```
```{r pt. 4 , include=TRUE, echo=TRUE}

## Aplicação dos algoritmos

################ Random Forest #########################

rf_model <- rand_forest(trees = 500,
                        min_n = 5, 
                        mtry = sqrt(ncol(train_data) - 1), 
                        mode = "classification") %>%
  set_engine("randomForest")


rf_fit_cv <- fit_resamples(rf_model, rec, resamples = cv_folds, metrics = metric_set(accuracy))
rf_fit_cv %>% collect_metrics()

################ XGBoost #########################

xgb_model <- boost_tree() %>%
  set_mode("classification") %>%
  set_engine("xgboost") %>% 
  set_args(trees = 1000,
           tree_depth = tune(),
           min_n = tune(),
           loss_reduction = tune(),
           sample_size = tune(),
           mtry = tune(),
           learn_rate = tune())

xgb_grid <- grid_latin_hypercube(
                                tree_depth(),
                                min_n(),
                                loss_reduction(),
                                sample_size = sample_prop(),
                                finalize(mtry(), train_data),
                                learn_rate(),
                                size = 50)

xgb_wf <- workflow() %>% 
          add_model(xgb_model) %>% 
          add_recipe(rec)

xgb_tuning <-tune_grid(xgb_wf,
                        resamples = cv_folds,
                        grid = xgb_grid,
                        metrics = metric_set(roc_auc),
                        control = control_grid(save_pred = TRUE))

xgb_best_grid <- select_best(xgb_tuning, metric = "roc_auc")
xgb_best_wf <- finalize_workflow(xgb_wf, xgb_best_grid)

xgb_final_wf <- last_fit(xgb_best_wf, split = data_split)
xgb_final_wf %>% collect_metrics()


#xgb_final_model <- fit(xgb_best_wf, data = data_split)

# Variable importance plots
xgb_best_wf %>% 
  fit(data = train_data) %>% 
  pull_workflow_fit() %>% 
  vip(geom = "point")
```