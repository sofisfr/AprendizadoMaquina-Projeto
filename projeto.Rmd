---
title: "Projeto Final"
author: |
        | Nome: Sofia Ferreira
        | E-mail: sofia.f@aluno.ufabc.edu.br
        
        | Nome: Pedro Henrique Donassan de Macedo
        | E-mail: pedro.donassan@aluno.ufabc.edu.br
        
        | Nome: Lucas Gonçalves de Oliveira 
        | E-mail: goncalves.o@aluno.ufabc.edu.br
        
        | Nome: Ricardo de Oliveira Carracci
        | E-mail: ricardo.carracci@aluno.ufabc.edu.br 
        
        | Nome: Micael Lucas da Silva
        | E-mail: micael.lucas@aluno.ufabc.edu.br
        
        | Nome: 
        | E-mail: 
        | (Não é preciso informar os RAs)
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T,
                      fig.align='center',
                      cache=TRUE,
                      out.width = "60%",
                      out.heigth = "60%",
                      warning=FALSE,
                      message=FALSE)
options(width =70)
```

#

```{r pt. 1, include=TRUE, echo=TRUE}

## Introdução

cat("1. Introdução\n\n")


cat("Em nosso mundo moderno acelerado e exigente, a importância da saúde do sono e seu profundo impacto em nosso bem-estar geral não podem ser exagerados. Isso por conta que o sono é um processo fisiológico fundamental que desempenha um papel vital na manutenção do equilíbrio físico, mental e emocional. \n
A qualidade e a quantidade do nosso sono influenciam diretamente nosso funcionamento diário, habilidades cognitivas, resiliência emocional e até mesmo nossa saúde a longo prazo. À medida que nos aprofundamos na intrincada relação entre saúde do sono e estilo de vida, descobrimos uma teia de fatores que contribuem para um sono reparador e exploramos como as escolhas de estilo de vida podem melhorar ou prejudicar a qualidade do nosso sono. \n
Logo, essa exploração abrange não apenas a ciência do sono, mas também os hábitos, rotinas e escolhas que coletivamente formam a base de nossa saúde durante o sono. Do ambiente de sono que criamos aos hábitos que cultivamos, entender a conexão simbiótica entre sono e estilo de vida nos capacita a tomar decisões informadas que podem levar a um melhor bem-estar e maior qualidade de vida.
\n")

```

```{r Libraries}
library(tidyverse)
library(tidymodels)
library(mlbench)
library(GGally)
library(corrplot)
library(vip)
library(janitor)
library(rpart.plot)
library(randomForest)
library(class)
library(caret)
```

```{r pt. 2 , include=TRUE, echo=TRUE}
cat("2. Exploração do Dataset\n\n")


cat("Seguindo o contexto da introdução apresentada anteriormente, o dataset escolhido para o desenvolvimento deste projeto  apresenta  um total de 400 linhas e 13 colunas com uma ampla gama de variáveis relacionadas ao sono e aos hábitos diários. Dentre os parâmetros, estão inclusos alguns como sexo, idade, ocupação, duração do sono, qualidade do sono, nível de atividade física, níveis de estresse, categoria de IMC, pressão arterial, frequência cardíaca, passos diários e presença ou ausência de distúrbios do sono.\n")


cat("A obtenção dos dados foi através da plataforma https://www.kaggle.com/datasets
link: https://www.kaggle.com/datasets/uom190346a/sleep-health-and-lifestyle-dataset")
```

```{r pt. 3 , include=TRUE, echo=TRUE}
cat("Em um primeiro momento, a partir da leitura do arquivo em CSV obtido da plataforma Kaggle, é realizada a formatação dos nomes das colunas, separa a coluna de pressão sanguínea em duas colunas numéricas (pressão sistólica e diastólica) e converte várias colunas em fatores. Em seguida, ele cria um tibble a partir dos dados e usa a função `ggpairs` para criar uma matriz de gráficos de dispersão das colunas especificadas. Finalmente, ele remove quaisquer linhas com valores ausentes, seleciona apenas as colunas numéricas, calcula a matriz de correlação e a visualiza usando a função `corrplot`.

O resultado da função `ggpairs` é uma matriz de gráficos de dispersão que mostra as relações entre pares das colunas especificadas. Cada célula na matriz exibe um gráfico de dispersão de duas variáveis, com uma variável no eixo x e outra no eixo y. As células diagonais mostram histogramas de cada variável. Este gráfico pode ajudá-lo a explorar visualmente as relações entre as variáveis e identificar quaisquer padrões ou tendências.

O resultado da função `corrplot` é uma visualização da matriz de correlação. A matriz de correlação mostra os coeficientes de correlação entre pares de todas as variáveis numéricas no conjunto de dados. A função `corrplot` cria uma exibição gráfica desta matriz, onde cada célula representa a correlação entre duas variáveis. A cor e o tamanho de cada célula indicam a força e a direção da correlação: azul para correlações positivas, vermelho para correlações negativas e branco para nenhuma correlação. Quanto mais próxima uma célula estiver do azul brilhante ou do vermelho brilhante, mais forte será a correlação. Este gráfico pode ajudá-lo a identificar rapidamente quais variáveis estão fortemente correlacionadas entre si.")

data <- read.csv(
    "Sleep_health_and_lifestyle_dataset.csv", 
    header=TRUE, 
    sep = ",", 
    row.names = NULL, 
    quote = "\"") 

data <- data %>% clean_names() #Formatando nome das colunas

data <- separate(data, blood_pressure, into = c("systolic_pressure", "diastolic_pressure"), sep = "/") #Transformando pressão sanguínea em 2 variáveis numericas
data$systolic_pressure <- as.numeric(data$systolic_pressure)
data$diastolic_pressure <- as.numeric(data$diastolic_pressure)

data$gender <- as.factor(data$gender)
data$occupation <- as.factor(data$occupation)
data$bmi_category <- as.factor(data$bmi_category)
#data$blood_pressure <- as.factor(data$blood_pressure)
data$sleep_disorder <- as.factor(data$sleep_disorder)

SleepData <- as_tibble(data) 

g <- ggpairs(
  data = SleepData,
  columns = c("age", "sleep_duration", "quality_of_sleep", "physical_activity_level", "stress_level", "heart_rate", "daily_steps", "systolic_pressure", "diastolic_pressure"),
  title = "quality_of_sleep",
  upper = list(continuous = wrap("cor", size = 3), 
               combo = "box_no_facet", 
               discrete = "facetbar", 
               na = "na"),
  lower = list(continuous = "points", 
               combo = "facethist", 
               discrete = "facetbar", 
               na = "na"),
  diag = list(continuous = "densityDiag", 
              discrete = "barDiag",
              na = "naDiag"),
  axisLabels = c("show", "internal", "none")
)

cat(g, vp=viewport(angle=90, width = unit(4.5, "inches"), height = unit(4.5, "inches")))

data %>% na.omit() %>% select_if(is.numeric) %>% cor() %>% corrplot::corrplot(tl.cex = 0.5) 

#Printando matriz de correlação
```
```{r  pt. 4 , include=TRUE, echo=TRUE}

#Split para todos os modelos
set.seed(123) # para reprodutibilidade
data_split <- initial_split(SleepData, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)

#Cv para todos os modelos
set.seed(123) # para reprodutibilidade
cv_folds <- vfold_cv(train_data, v = 5) # 5 Folds pois o dataset é pequeno

#Recipe para todos os modelos
rec <- recipe(sleep_disorder ~ ., data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>% 
  step_rm(person_id) %>%  ### Remove a coluna person ID que é inutil para o modelo
  step_corr(all_numeric()) ### Remove variaveis altamente correlacionadas

```

```{r pt. 5 , include=TRUE, echo=TRUE}

cat("No desenvolvimento da análise a seguir utilizamos três algoritimos deiferentes afim de gerar uma maior abordagem em nosso conjunto de dados. São eles o algoritomo de KNN, Random Florest e o algoritimo de XGBoost.")

cat("\n\nKNN - K-Nearest Neighbors\n")

cat("O KNN (K-Nearest Neighbors) é um algoritmo de aprendizado de máquina usado tanto para classificação quanto para regressão. Ele se baseia no princípio de que objetos semelhantes tendem a estar próximos uns dos outros em um espaço de características. Quando se trata de tomar decisões sobre novos pontos, o KNN considera a opinião da maioria dos pontos próximos a esses novos pontos.\n")

cat("A ideia central por trás do KNN é que objetos semelhantes tendem a estar próximos uns dos outros em um espaço de características. Portanto, a classificação ou predição de um novo ponto é feita com base na maioria das classes (no caso de classificação) ou nos valores médios (no caso de regressão) dos K pontos mais próximos a ele no espaço de características.\n")

cat("Esse método é o mais simples usado na análise dos dados desse trabalho, contudo é muito comum o mesmo ser uma primeira abordagem em um aprendizado supervisionado, pois, a principal determinação a ser feita é do seu hiperparametro K, que define a qualidade da predição feita pelo algoritimo.\n")

cat("Passo a Passo:\n")

cat("Passo 1: Selecionar o número K dos vizinhos.\n")
cat("Passo 2: Calcular a distância euclidiana dos K vizinhos.\n")
cat("Passo 3: Selecionar os K vizinhos mais próximos de acordo com a distância euclidiana calculada.\n")
cat("Passo 4: Entre esses K vizinhos, contar o número de pontos de dados em cada categoria.\n")
cat("Passo 5: Atribuir os novos pontos de dados à categoria para a qual o número de vizinhos é máximo.\n")

cat("\n\nFlorestas Aleatórias\n")

cat("Florestas Aleatórias é um algoritmo de aprendizado de máquina que opera construindo uma multiplicidade de árvores de decisão durante o tempo de treinamento. É um método de aprendizado de conjunto para classificação, regressão e outras tarefas. Para tarefas de classificação, a saída do algortimo é a classe selecionada pela maioria das árvores. Florestas Aleatórias geralmente superam as árvores de decisão, mas sua precisão é menor do que as árvores impulsionadas pelo gradiente.\n")

cat("A ideia principal por trás do algoritmo Random Forest é combinar a sabedoria coletiva de múltiplas árvores de decisão para melhorar a precisão e a robustez das previsões. Ele se baseia em dois conceitos principais: diversidade e média ponderada.\n")

cat("Passo a Passo:\n")

cat("Passo 1: Selecionar K pontos de dados aleatórios do conjunto de treinamento.\n")
cat("Passo 2: Construir as árvores de decisão associadas aos pontos de dados selecionados (Subconjuntos).\n")
cat("Passo 3: Escolher o número N para as árvores de decisão que você deseja construir.\n")
cat("Passo 4: Repetir os Passos 1 e 2.\n")
cat("Passo 5: Para novos pontos de dados, encontrar as previsões de cada árvore de decisão e atribuir os novos pontos de dados à categoria que vencer a maioria dos votos.\n")

cat("\n\nXGBoost - Extreme Gradient Boosting\n")

cat("Extreme Gradient Boosting (XGBoost) é um algoritmo de aprendizado de máquina amplamente utilizado para tarefas de regressão e classificação. Ele é uma extensão do algoritmo de boosting tradicional, projetado para melhorar a eficácia e eficiência do boosting, especialmente em termos de desempenho e velocidade de treinamento.\n")

cat("O boosting mencionado acima é uma técnica de aprendizado de máquina em que vários modelos fracos (geralmente árvores de decisão rasas) são treinados sequencialmente, cada um corrigindo os erros do modelo anterior. Os modelos fracos são combinados para criar um modelo forte que é mais capaz de fazer previsões precisas.\n")

cat("Portanto em resumo, o XGBoost é uma implementação otimizada do algoritmo de gradient boosting, uma técnica que combina várias árvores de decisão fracas para formar um modelo forte. Cada árvore subsequente é treinada para corrigir os erros do modelo anterior. A combinação das previsões de todas as árvores individuais resulta em uma previsão final robusta e precisa.\n")

cat("O XGBoost é uma escolha sólida para problemas de classificação devido à sua eficiência, desempenho, capacidade de lidar com complexidade e regularização embutida, o que resulta em modelos mais precisos e robustos.\n")

cat("Passo a Passo:\n")

cat("Passo 1: Comece com um modelo básico, como uma árvore de decisão.\n")
cat("Passo 2: Treine o modelo para aprender com os erros que comete.\n")
cat("Passo 3: Adicione mais modelos simples e foque nos erros anteriores.\n")
cat("Passo 4: Repita o treinamento e adição de modelos várias vezes.\n")
cat("Passo 5: Combine as respostas de todos os modelos para a decisão final.\n")

```



```{r pt. 6 , include=TRUE, echo=TRUE}

## Aplicação dos algoritmos

################ KNN #########################

trControl <- trainControl(method  = "cv",
                          number  = 5)

fit <- train(sleep_disorder ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = train_data)

fit

test_pred <- knn(
  train_data[,c("sleep_duration","systolic_pressure","diastolic_pressure")], 
  test_data[,c("sleep_duration","systolic_pressure","diastolic_pressure")],
  train_data$sleep_disorder, 
  k=1
)

table(test_data$sleep_disorder, test_pred)

################ Random Forest #########################

rf_model <- rand_forest(trees = 500,
                        min_n = 5, 
                        mtry = sqrt(ncol(train_data) - 1), 
                        mode = "classification") %>%
  set_engine("randomForest")


rf_fit_cv <- fit_resamples(rf_model, rec, resamples = cv_folds, metrics = metric_set(accuracy))
rf_fit_cv %>% collect_metrics()

################ XGBoost #########################

xgb_model <- boost_tree() %>%
  set_mode("classification") %>%
  set_engine("xgboost") %>% 
  set_args(trees = 1000,
           tree_depth = tune(),
           min_n = tune(),
           loss_reduction = tune(),
           sample_size = tune(),
           mtry = tune(),
           learn_rate = tune())

xgb_grid <- grid_latin_hypercube(
                                tree_depth(),
                                min_n(),
                                loss_reduction(),
                                sample_size = sample_prop(),
                                finalize(mtry(), train_data),
                                learn_rate(),
                                size = 50)

xgb_wf <- workflow() %>% 
          add_model(xgb_model) %>% 
          add_recipe(rec)

xgb_tuning <-tune_grid(xgb_wf,
                        resamples = cv_folds,
                        grid = xgb_grid,
                        metrics = metric_set(roc_auc),
                        control = control_grid(save_pred = TRUE))

xgb_best_grid <- select_best(xgb_tuning, metric = "roc_auc")
xgb_best_wf <- finalize_workflow(xgb_wf, xgb_best_grid)

xgb_final_wf <- last_fit(xgb_best_wf, split = data_split)
xgb_final_wf %>% collect_metrics()


#xgb_final_model <- fit(xgb_best_wf, data = data_split)

# Variable importance plots
xgb_best_wf %>% 
  fit(data = train_data) %>% 
  pull_workflow_fit() %>% 
  vip(geom = "point")
```
